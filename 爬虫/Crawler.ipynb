{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最简单的爬取方式\n",
    "\n",
    "Derivative from [Morvan Python-爬虫](https://github.com/MorvanZhou/easy-scraping-tutorial/tree/master/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Scraping tutorial 1 | 莫烦Python</title>\n",
      "\t<link rel=\"icon\" href=\"https://morvanzhou.github.io/static/img/description/tab_icon.png\">\n",
      "</head>\n",
      "<body>\n",
      "\t<h1>爬虫测试1</h1>\n",
      "\t<p>\n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# 如果有中文的话用.decode解码\n",
    "html = urlopen(\n",
    "    \"https://morvanzhou.github.io/static/scraping/basic-structure.html\"\n",
    "    ).read().decode('utf-8')\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用正则表达式选取文本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page title is:  Scraping tutorial 1 | 莫烦Python\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "res = re.findall(r'<title>(.+?)</title>',html)\n",
    "print(\"\\nPage title is: \",res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page paragraph is:  \n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "res = re.findall(r\"<p>(.*?)</p>\", html, flags=re.DOTALL)    # re.DOTALL if multi line\n",
    "print(\"\\nPage paragraph is: \", res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All links:  ['https://morvanzhou.github.io/static/img/description/tab_icon.png', 'https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/data-manipulation/scraping/']\n"
     ]
    }
   ],
   "source": [
    "# 抓取链接\n",
    "res = re.findall(r'href=\"(.*?)\"', html)\n",
    "print(\"\\nAll links: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "高级匹配\n",
    "\n",
    "中文官网 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Scraping tutorial 1 | 莫烦Python</title>\n",
      "\t<link rel=\"icon\" href=\"https://morvanzhou.github.io/static/img/description/tab_icon.png\">\n",
      "</head>\n",
      "<body>\n",
      "\t<h1>爬虫测试1</h1>\n",
      "\t<p>\n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://morvanzhou.github.io/static/scraping/basic-structure.html\").read().decode('utf-8')\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>爬虫测试1</h1>\n",
      "\n",
      " <p>\n",
      "\t\t这是一个在 <a href=\"https://morvanzhou.github.io/\">莫烦Python</a>\n",
      "<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a> 中的简单测试.\n",
      "\t</p>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, features='lxml') # lxml是一种解析形式\n",
    "print(soup.h1)  # 选取tag h1\n",
    "print('\\n', soup.p)  # 选取tag p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://morvanzhou.github.io/\">莫烦Python</a>, <a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">爬虫教程</a>]\n",
      "\n",
      " ['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/data-manipulation/scraping/']\n"
     ]
    }
   ],
   "source": [
    "all_href = soup.find_all('a') # 找出所有a的tag，这里a是tag\n",
    "print(all_href)\n",
    "all_href = [l['href'] for l in all_href] # 提取其中的网址部分，这里href是属性\n",
    "print('\\n', all_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://morvanzhou.github.io/\n",
      "https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\n"
     ]
    }
   ],
   "source": [
    "# 或者\n",
    "all_href = soup.find_all('a')\n",
    "for i in all_href:\n",
    "    print(i['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautifulsoup: find by CSS class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"cn\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>爬虫练习 列表 class | 莫烦 Python</title>\n",
      "\t<style>\n",
      "\t.jan {\n",
      "\t\tbackground-color: yellow;\n",
      "\t}\n",
      "\t.feb {\n",
      "\t\tfont-size: 25px;\n",
      "\t}\n",
      "\t.month {\n",
      "\t\tcolor: red;\n",
      "\t}\n",
      "\t</style>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "<h1>列表 爬虫练习</h1>\n",
      "\n",
      "<p>这是一个在 <a href=\"https:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"https://morvanzhou.github.io/static/scraping/list.html\").read().decode('utf-8')\n",
    "print(html[:280])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上述html中CSS代码部分为:\n",
    "\n",
    "\t<style\\>\n",
    "\t.jan {\n",
    "\t\tbackground-color: yellow;\n",
    "\t}\n",
    "\t.feb {\n",
    "\t\tfont-size: 25px;\n",
    "\t}\n",
    "\t.month {\n",
    "\t\tcolor: red;\n",
    "\t}\n",
    "    </style\\>\n",
    "\n",
    "其中jan、feb、month为class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页源代码\n",
    "\n",
    "<img src=\"web_source_code.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一月\n",
      "二月\n",
      "三月\n",
      "四月\n",
      "五月\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "# use class to narrow search\n",
    "month = soup.find_all('li', {\"class\": \"month\"}) \n",
    "# 由于html中tag为li的部分还包含了<li>一月一号</li>,所以需要用class选出实际所需要的部分\n",
    "for m in month:\n",
    "    print(m.get_text()) # 最后m实际为<li class=\"month\">五月</li>，需要用.get_text()只提取文字部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul class=\"jan\">\n",
      "<li>一月一号</li>\n",
      "<li>一月二号</li>\n",
      "<li>一月三号</li>\n",
      "</ul> \n",
      "\n",
      "[<li>一月一号</li>, <li>一月二号</li>, <li>一月三号</li>] \n",
      "\n",
      "一月一号\n",
      "一月二号\n",
      "一月三号\n"
     ]
    }
   ],
   "source": [
    "# 第二个例子\n",
    "jan = soup.find('ul', {\"class\": 'jan'})\n",
    "print(jan,\"\\n\")\n",
    "\n",
    "d_jan = jan.find_all('li')  \n",
    "print(d_jan,\"\\n\")\n",
    "\n",
    "for d in d_jan:\n",
    "    print(d.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautifulsoup: find by regular expression（正则表达）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tTensorflow 神经网络</a>\n",
      "\t\t</td><td>\n",
      "\t\t\t2:00\n",
      "\t\t</td><td>\n",
      "\t\t\t<img src=\"https://morvanzhou.github.io/static/img/course_cover/tf.jpg\">\n",
      "\t\t</td>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr id=\"course2\" class=\"ml\">\n",
      "\t\t<td>\n",
      "\t\t\t机器学习\n",
      "\t\t</td><td>\n",
      "\t\t\t<a href=\"https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\">\n",
      "\t\t\t\t强化学习</a>\n",
      "\t\t</td><td>\n",
      "\t\t\t5:00\n",
      "\t\t</td><td>\n",
      "\t\t\t<img src=\"https://morvanzhou.github.io/static/img/course_cover/rl.jpg\">\n",
      "\t\t</td>\n",
      "\t</tr>\n",
      "\n",
      "\t<tr id=\"course3\" class=\"data\">\n",
      "\t\t<td>\n",
      "\t\t\t数据处理\n",
      "\t\t</td><td>\n",
      "\t\t\t<a href=\"https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\">\n",
      "\t\t\t\t爬虫</a>\n",
      "\t\t</td><td>\n",
      "\t\t\t3:00\n",
      "\t\t</td><td>\n",
      "\t\t\t<img src=\"https://morvanzhou.github.io/static/img/course_cover/scraping.jpg\">\n",
      "\t\t</td>\n",
      "\t</tr>\n",
      "\n",
      "</table>\n",
      "\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "html = urlopen(\"https://morvanzhou.github.io/static/scraping/table.html\").read().decode('utf-8')\n",
    "print(html[700:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://morvanzhou.github.io/static/img/course_cover/tf.jpg\n",
      "https://morvanzhou.github.io/static/img/course_cover/rl.jpg\n",
      "https://morvanzhou.github.io/static/img/course_cover/scraping.jpg\n"
     ]
    }
   ],
   "source": [
    "# 提取jpg图片\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "img_links = soup.find_all(\"img\", {\"src\": re.compile('.*?\\.jpg')})\n",
    "for link in img_links:\n",
    "    print(link['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://morvanzhou.github.io/\n",
      "https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\n",
      "https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/\n",
      "https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n",
      "https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\n"
     ]
    }
   ],
   "source": [
    "course_links = soup.find_all('a', {'href': re.compile('https://morvan.*')})\n",
    "for link in course_links:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice: scrape Baidu Baike\n",
    "- https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n",
    "<img src=\"page.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "base_url = \"https://baike.baidu.com\"\n",
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n"
     ]
    }
   ],
   "source": [
    "url = base_url + his[-1]\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "print(soup.find('h1').get_text(), '    url: ', his[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all sub_urls（超链接） for baidu baike (item page), randomly select a sub_urls and store it in \"his\". If no valid sub link is found, than pop last url in \"his\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"/item/%E4%B8%87%E7%BB%B4%E7%BD%91\" target=\"_blank\">万维网</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C\" target=\"_blank\">网络</a>, <a href=\"/item/%E4%B8%87%E7%BB%B4%E7%BD%91\" target=\"_blank\">万维网</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E9%80%9A%E7%94%A8%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">通用搜索引擎</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE\" target=\"_blank\">网络数据</a>, <a href=\"/item/%E4%B8%87%E7%BB%B4%E7%BD%91\" target=\"_blank\">万维网</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%8A%80%E6%9C%AF\" target=\"_blank\">网络技术</a>, <a href=\"/item/%E9%80%9A%E7%94%A8%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">通用搜索引擎</a>, <a href=\"/item/%E5%85%B3%E9%94%AE%E5%AD%97\" target=\"_blank\">关键字</a>, <a href=\"/item/%E4%B8%87%E7%BB%B4%E7%BD%91\" target=\"_blank\">万维网</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5\" target=\"_blank\">搜索策略</a>, <a href=\"/item/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95\" target=\"_blank\">排序算法</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96\" target=\"_blank\">网页抓取</a>, <a href=\"/item/%E7%88%AC%E8%99%AB\" target=\"_blank\">爬虫</a>, <a href=\"/item/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2\" target=\"_blank\">广度优先搜索</a>, <a href=\"/item/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E7%AD%96%E7%95%A5\" target=\"_blank\">广度优先策略</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95\" target=\"_blank\">搜索算法</a>, <a href=\"/item/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E7%AD%96%E7%95%A5\" target=\"_blank\">深度优先策略</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91\" target=\"_blank\">网络拓扑</a>, <a href=\"/item/%E9%80%92%E5%BD%92\" target=\"_blank\">递归</a>, <a href=\"/item/%E4%B8%BB%E6%9C%BA\" target=\"_blank\">主机</a>, <a href=\"/item/%E9%94%9A%E6%96%87%E6%9C%AC\" target=\"_blank\">锚文本</a>, <a href=\"/item/%E7%9F%A9%E9%98%B5\" target=\"_blank\">矩阵</a>, <a href=\"/item/%E8%B6%85%E6%96%87%E6%9C%AC\" target=\"_blank\">超文本</a>, <a href=\"/item/%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2\" target=\"_blank\">动态页面</a>, <a href=\"/item/%E6%96%87%E6%9C%AC%E6%A3%80%E7%B4%A2\" target=\"_blank\">文本检索</a>, <a href=\"/item/%E6%95%B0%E6%8D%AE%E6%8A%BD%E5%8F%96\" target=\"_blank\">数据抽取</a>, <a href=\"/item/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98\" target=\"_blank\">数据挖掘</a>, <a href=\"/item/%E8%B6%85%E9%93%BE%E6%8E%A5\" target=\"_blank\">超链接</a>, <a href=\"/item/%E6%95%B0%E6%8D%AE%E6%BA%90\" target=\"_blank\">数据源</a>, <a href=\"/item/%E6%96%87%E6%9C%AC%E6%A3%80%E7%B4%A2\" target=\"_blank\">文本检索</a>, <a href=\"/item/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90\" target=\"_blank\">文本分析</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6\" target=\"_blank\">垃圾邮件</a>, <a href=\"/item/%E8%B6%85%E9%93%BE%E6%8E%A5\" target=\"_blank\">超链接</a>, <a href=\"/item/%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2\" target=\"_blank\">动态页面</a>, <a href=\"/item/%E8%84%9A%E6%9C%AC%E8%AF%AD%E8%A8%80\" target=\"_blank\">脚本语言</a>, <a href=\"/item/%E7%88%B1%E5%BE%B7%E5%8D%8E\" target=\"_blank\">爱德华</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E5%8A%B3%E4%BC%A6%E6%96%AF\" target=\"_blank\">劳伦斯</a>, <a href=\"/item/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6\" target=\"_blank\">斯坦福大学</a>, <a href=\"/item/%E4%B8%BB%E6%9C%BA\" target=\"_blank\">主机</a>, <a href=\"/item/%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E7%AD%96%E7%95%A5\" target=\"_blank\">广度优先策略</a>, <a href=\"/item/%E8%BF%AD%E4%BB%A3\" target=\"_blank\">迭代</a>, <a href=\"/item/%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90\" target=\"_blank\">随机种子</a>, <a href=\"/item/%E9%94%9A%E7%82%B9\" target=\"_blank\">锚点</a>, <a href=\"/item/%E8%B6%85%E6%96%87%E6%9C%AC\" target=\"_blank\">超文本</a>, <a href=\"/item/%E8%AF%AD%E4%B9%89%E7%BD%91%E7%BB%9C\" target=\"_blank\">语义网络</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E8%B7%AF%E7%94%B1%E5%99%A8\" target=\"_blank\">路由器</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">网络服务器</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">网络服务器</a>, <a href=\"/item/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1\" target=\"_blank\">系统设计</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95\" target=\"_blank\">排序算法</a>, <a href=\"/item/%E5%9E%83%E5%9C%BE%E9%82%AE%E4%BB%B6\" target=\"_blank\">垃圾邮件</a>, <a href=\"/item/%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86\" target=\"_blank\">用户代理</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">网络服务器</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%91%98\" target=\"_blank\">网络管理员</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%91%98\" target=\"_blank\">网络管理员</a>, <a href=\"/item/%E7%94%A8%E6%88%B7%E6%A0%87%E8%AF%86\" target=\"_blank\">用户标识</a>, <a href=\"/item/%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95\" target=\"_blank\">全文索引</a>, <a href=\"/item/%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F\" target=\"_blank\">爬虫程序</a>, <a href=\"/item/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E7%AD%96%E7%95%A5\" target=\"_blank\">深度优先策略</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE\" target=\"_blank\">网络数据</a>, <a href=\"/item/%E4%B8%8B%E8%BD%BD%E8%80%85\" target=\"_blank\">下载者</a>, <a href=\"/item/%E6%89%B9%E5%A4%84%E7%90%86\" target=\"_blank\">批处理</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">网络服务器</a>, <a href=\"/item/%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">代理服务器</a>, <a href=\"/item/%E4%B8%BB%E6%9C%BA\" target=\"_blank\">主机</a>, <a href=\"/item/%E7%BD%91%E9%A1%B5%E6%8A%93%E5%8F%96\" target=\"_blank\">网页抓取</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E6%9C%8D%E5%8A%A1%E5%99%A8\" target=\"_blank\">网络服务器</a>, <a href=\"/item/%E9%95%9C%E5%83%8F\" target=\"_blank\">镜像</a>, <a href=\"/item/%E4%BA%92%E8%81%94%E7%BD%91%E6%A1%A3%E6%A1%88%E9%A6%86\" target=\"_blank\">互联网档案馆</a>, <a href=\"/item/%E7%BD%91%E7%BB%9C%E7%AB%99%E7%82%B9\" target=\"_blank\">网络站点</a>, <a href=\"/item/%E7%A6%BB%E7%BA%BF%E8%A7%82%E7%9C%8B\" target=\"_blank\">离线观看</a>, <a href=\"/item/%E9%95%9C%E5%83%8F\" target=\"_blank\">镜像</a>, <a href=\"/item/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F\" target=\"_blank\">正则表达式</a>, <a href=\"/item/%E6%96%AF%E5%9D%A6%E7%A6%8F\" target=\"_blank\">斯坦福</a>, <a href=\"/item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">搜索引擎</a>, <a href=\"/item/%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2\" target=\"_blank\">图形用户界面</a>, <a href=\"/item/%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81\" target=\"_blank\">开放源代码</a>, <a href=\"/item/%E6%8D%B7%E5%85%8B\" target=\"_blank\">捷克</a>, <a href=\"/item/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\" target=\"_blank\">分布式搜索引擎</a>, <a href=\"/item/%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81\" target=\"_blank\">开放源代码</a>, <a href=\"/item/%E8%B6%85%E9%93%BE%E6%8E%A5\" target=\"_blank\">超链接</a>]\n",
      "['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E8%B6%85%E9%93%BE%E6%8E%A5', '/item/%E4%B8%87%E7%BB%B4%E7%BD%91', '/item/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86%E5%91%98']\n"
     ]
    }
   ],
   "source": [
    "# 随机选取一个超链接，如“万维网”\n",
    "sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "print(sub_urls)\n",
    "\n",
    "if len(sub_urls) != 0:\n",
    "    his.append(random.sample(sub_urls, 1)[0]['href'])  # 随机选取一个加到his里面\n",
    "else:\n",
    "    # no valid sub link found\n",
    "    his.pop()\n",
    "print(his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p3.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put everthing together. Random running for 20 iterations. See what we end up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n",
      "1 搜索引擎     url:  /item/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "2 全球信息网格     url:  /item/%E5%85%A8%E7%90%83%E4%BF%A1%E6%81%AF%E7%BD%91%E6%A0%BC\n",
      "3 服务器     url:  /item/%E6%9C%8D%E5%8A%A1%E5%99%A8\n",
      "4 大脑     url:  /item/%E5%A4%A7%E8%84%91\n",
      "5 中央沟     url:  /item/%E4%B8%AD%E5%A4%AE%E6%B2%9F\n",
      "6 额叶     url:  /item/%E9%A2%9D%E5%8F%B6\n",
      "7 运动障碍     url:  /item/%E8%BF%90%E5%8A%A8%E9%9A%9C%E7%A2%8D\n",
      "8 下运动神经元     url:  /item/%E4%B8%8B%E8%BF%90%E5%8A%A8%E7%A5%9E%E7%BB%8F%E5%85%83\n",
      "9 腱反射     url:  /item/%E6%B7%B1%E5%8F%8D%E5%B0%84\n",
      "10 前臂     url:  /item/%E5%89%8D%E8%87%82\n",
      "11 肌肉组织     url:  /item/%E8%82%8C%E8%82%89%E7%BB%84%E7%BB%87\n",
      "12 安德烈·海姆     url:  /item/%E6%B5%B7%E5%A7%86\n",
      "13 英国皇家学会     url:  /item/%E7%9A%87%E5%AE%B6%E5%AD%A6%E4%BC%9A\n",
      "14 西蒙·唐纳森     url:  /item/%E8%A5%BF%E8%92%99%C2%B7%E5%94%90%E7%BA%B3%E6%A3%AE\n",
      "15 伦敦     url:  /item/%E4%BC%A6%E6%95%A6\n",
      "16 泰晤士河     url:  /item/%E6%B3%B0%E6%99%A4%E5%A3%AB%E6%B2%B3\n",
      "17 地中海     url:  /item/%E5%9C%B0%E4%B8%AD%E6%B5%B7\n",
      "18 古巴比伦王国     url:  /item/%E5%8F%A4%E5%B7%B4%E6%AF%94%E4%BC%A6\n",
      "19 巴比伦第一王朝     url:  /item/%E5%B7%B4%E6%AF%94%E4%BC%A6%E7%AC%AC%E4%B8%80%E7%8E%8B%E6%9C%9D\n"
     ]
    }
   ],
   "source": [
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]\n",
    "\n",
    "for i in range(20):\n",
    "    url = base_url + his[-1]  # 进入下一个超链接，循环....\n",
    "\n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    print(i, soup.find('h1').get_text(), '    url: ', his[-1])\n",
    "\n",
    "    # find valid urls\n",
    "    sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "\n",
    "    if len(sub_urls) != 0:\n",
    "        his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "    else:\n",
    "        # no valid sub link found\n",
    "        his.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests: an alternative to urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在加载网页的时候, 有几种类型, 而这几种类型就是打开网页的关键. 最重要的类型 (method) 就是 get 和 post (当然还有其他的, 比如 head, delete).\n",
    "\n",
    "#### post\n",
    "- 账号登录\n",
    "- 搜索内容\n",
    "- 上传图片\n",
    "- 上传文件\n",
    "- 往服务器传数据\n",
    "\n",
    "#### get\n",
    "- 正常打开网页\n",
    "- 不往服务器传数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=Python%E7%88%AC%E8%99%AB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import webbrowser\n",
    "param = {\"wd\": \"Python爬虫\"}\n",
    "r = requests.get('http://www.baidu.com/s', params=param)\n",
    "print(r.url)\n",
    "webbrowser.open(r.url) # 在浏览器端打开这个网页"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p4.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there, Mengyu Ge!\n"
     ]
    }
   ],
   "source": [
    "data = {'firstname': 'Mengyu', 'lastname': 'Ge'}\n",
    "r = requests.post('http://pythonscraping.com/pages/files/processing.php', data=data)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"p5.png\" style=\"width:400px;height:200px;float:right\">\n",
    "\n",
    "<img src=\"p6.png\" style=\"width:400px;height:150px;float:left\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"p7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload image\n",
    "\n",
    "We still use post function to update image in this [page](http://pythonscraping.com/files/form2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploads/\n",
      "Sorry, there was an error uploading your file.\n"
     ]
    }
   ],
   "source": [
    "file = {'uploadFile': open('p2.png', 'rb')}\n",
    "r = requests.post('http://pythonscraping.com/files/processing2.php', files=file)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## login\n",
    "Use post method to login to a [website](http://pythonscraping.com/pages/cookies/login.html)\n",
    "\n",
    "这个网有点炸，所以有的情况下不会正常工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loggedin': '1', 'username': 'Mengyu'}\n"
     ]
    }
   ],
   "source": [
    "payload = {'username': 'Mengyu', 'password': 'password'}\n",
    "r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)\n",
    "# ↑第二次登陆就直接用Cookie\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another general way to login\n",
    "\n",
    "Use session instead requests. Keep you in a session and keep track the cookies.\n",
    "\n",
    "（用一系列对话来控制cookies, requests.Session()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loggedin': '1', 'username': 'Mengyu'}\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "payload = {'username': 'Mengyu', 'password': 'password'}\n",
    "r = session.post('http://pythonscraping.com/pages/cookies/welcome.php', data=payload)\n",
    "print(r.cookies.get_dict())\n",
    "r = session.get(\"http://pythonscraping.com/pages/cookies/profile.php\")\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载图片\n",
    "import os\n",
    "os.makedirs('./img/', exist_ok=True)\n",
    "\n",
    "IMAGE_URL = \"https://morvanzhou.github.io/static/img/description/learning_step_flowchart.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用urlretrieve下载图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./img/image1.png', <http.client.HTTPMessage at 0x265363ad828>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve(IMAGE_URL, './img/image1.png')      # whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用requests下载图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(IMAGE_URL)\n",
    "with open('./img/image2.png', 'wb') as f:\n",
    "    f.write(r.content)                      # whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述两种方法都是先下载到内存里面，再存到本地，不适合下载大批量文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download chunck by chunck\n",
    "\n",
    "Set stream = True in get() function. 这样更高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(IMAGE_URL, stream=True)    # stream loading\n",
    "\n",
    "with open('./img/image3.png', 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=32):\n",
    "        f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images from web\n",
    "\n",
    "Download amazing pictures from [national geographic](http://www.ngchina.com.cn/animals/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "URL = \"http://www.ngchina.com.cn/animals/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find list of image holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='p9.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(URL).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "img_ul = soup.find_all('ul', {\"class\": \"img_list\"})\n",
    "\n",
    "import os\n",
    "os.makedirs('./img2/', exist_ok=True)\n",
    "\n",
    "for ul in img_ul:\n",
    "    imgs = ul.find_all('img')\n",
    "    for img in imgs:\n",
    "        url = img['src']\n",
    "        r = requests.get(url, stream=True)\n",
    "        image_name = url.split('/')[-1]\n",
    "        with open('./img2/%s' % image_name, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=128):\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed scraping: multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "from urllib.request import urlopen, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "base_url = 'https://morvanzhou.github.io/'\n",
    "\n",
    "# DON'T OVER CRAWL THE WEBSITE OR YOU MAY NEVER VISIT AGAIN\n",
    "if base_url != \"http://127.0.0.1:4000/\":\n",
    "    restricted_crawl = True\n",
    "else:\n",
    "    restricted_crawl = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = urlopen(url)\n",
    "    time.sleep(0.1)             # slightly delay for downloading\n",
    "    return response.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    urls = soup.find_all('a', {\"href\": re.compile('^/.+?/$')})\n",
    "    title = soup.find('h1').get_text().strip()\n",
    "    page_urls = set([urljoin(base_url, url['href']) for url in urls])\n",
    "    url = soup.find('meta', {'property': \"og:url\"})['content']\n",
    "    return title, page_urls, url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正常不使用multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distributed Crawling...\n",
      "\n",
      "Distributed Parsing...\n",
      "\n",
      "Analysing...\n",
      "1 教程 https://morvanzhou.github.io/\n",
      "\n",
      "Distributed Crawling...\n",
      "\n",
      "Distributed Parsing...\n",
      "\n",
      "Analysing...\n",
      "2 进化算法 Evolutionary Strategies 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/evolutionary-algorithm/\n",
      "3 迁移学习 Transfer Learning https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-16-transfer-learning/\n",
      "4 机器学习实践 https://morvanzhou.github.io/tutorials/machine-learning/ML-practice/\n",
      "5 基础教程系列 https://morvanzhou.github.io/tutorials/python-basic/basic/\n",
      "6 Tkinter GUI 教程系列 https://morvanzhou.github.io/tutorials/python-basic/tkinter/\n",
      "7 multiprocessing 多进程教程系列 https://morvanzhou.github.io/tutorials/python-basic/multiprocessing/\n",
      "8 Numpy & Pandas 教程系列 https://morvanzhou.github.io/tutorials/data-manipulation/np-pd/\n",
      "9 Pytorch 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/torch/\n",
      "10 关于莫烦 https://morvanzhou.github.io/about/\n",
      "11 数据处理教程系列 https://morvanzhou.github.io/tutorials/data-manipulation/\n",
      "12 其他教学系列 https://morvanzhou.github.io/tutorials/others/\n",
      "13 推荐学习顺序 https://morvanzhou.github.io/learning-steps/\n",
      "14 Theano 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/theano/\n",
      "15 为了更优秀 https://morvanzhou.github.io/support/\n",
      "16 Sklearn 通用机器学习 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/sklearn/\n",
      "17 计算机视觉 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/computer-vision/\n",
      "18 网页爬虫教程系列 https://morvanzhou.github.io/tutorials/data-manipulation/scraping/\n",
      "19 Why? https://morvanzhou.github.io/tutorials/data-manipulation/scraping/1-00-why/\n",
      "20 迁移学习 Transfer Learning https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-9-transfer-learning/\n",
      "21 机器学习系列 https://morvanzhou.github.io/tutorials/machine-learning/\n",
      "22 Matplotlib 画图教程系列 https://morvanzhou.github.io/tutorials/data-manipulation/plt/\n",
      "23 Linux 简易教学 https://morvanzhou.github.io/tutorials/others/linux-basic/\n",
      "24 Tensorflow 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/\n",
      "25 高级爬虫: 让 Selenium 控制你的浏览器帮你爬 https://morvanzhou.github.io/tutorials/data-manipulation/scraping/5-01-selenium/\n",
      "26 说吧~ https://morvanzhou.github.io/discuss/\n",
      "27 有趣的机器学习系列 https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/\n",
      "28 近期更新 https://morvanzhou.github.io/recent-posts/\n",
      "29 高级爬虫: 高效无忧的 Scrapy 爬虫库 https://morvanzhou.github.io/tutorials/data-manipulation/scraping/5-02-scrapy/\n",
      "30 Keras 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/keras/\n",
      "31 Git 版本管理 教程系列 https://morvanzhou.github.io/tutorials/others/git/\n",
      "32 Threading 多线程教程系列 https://morvanzhou.github.io/tutorials/python-basic/threading/\n",
      "33 Python基础 教程系列 https://morvanzhou.github.io/tutorials/python-basic/\n",
      "34 强化学习 Reinforcement Learning 教程系列 https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n",
      "Total time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "unseen = set([base_url,])\n",
    "seen = set()\n",
    "\n",
    "count, t1 = 1, time.time()\n",
    "\n",
    "while len(unseen) != 0:                 # still get some url to visit\n",
    "    if restricted_crawl and len(seen) > 20:\n",
    "            break\n",
    "        \n",
    "    print('\\nDistributed Crawling...')\n",
    "    htmls = [crawl(url) for url in unseen]\n",
    "\n",
    "    print('\\nDistributed Parsing...')\n",
    "    results = [parse(html) for html in htmls]\n",
    "\n",
    "    print('\\nAnalysing...')\n",
    "    seen.update(unseen)         # seen the crawled\n",
    "    unseen.clear()              # nothing unseen\n",
    "\n",
    "    for title, page_urls, url in results:\n",
    "        print(count, title, url)\n",
    "        count += 1\n",
    "        unseen.update(page_urls - seen)     # get new url to crawl\n",
    "print('Total time: %.1f s' % (time.time()-t1, ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = set([base_url,])\n",
    "seen = set()\n",
    "\n",
    "pool = mp.Pool(4)                       \n",
    "count, t1 = 1, time.time()\n",
    "while len(unseen) != 0:                 # still get some url to visit\n",
    "    if restricted_crawl and len(seen) > 20:\n",
    "            break\n",
    "    print('\\nDistributed Crawling...')\n",
    "    crawl_jobs = [pool.apply_async(crawl, args=(url,)) for url in unseen]  # +++++++++\n",
    "    htmls = [j.get() for j in crawl_jobs]                                       # request connection\n",
    "\n",
    "    print('\\nDistributed Parsing...')\n",
    "    parse_jobs = [pool.apply_async(parse, args=(html,)) for html in htmls]  # +++++++++\n",
    "    results = [j.get() for j in parse_jobs]                                     # parse html\n",
    "\n",
    "    print('\\nAnalysing...')\n",
    "    seen.update(unseen)         # seen the crawled\n",
    "    unseen.clear()              # nothing unseen\n",
    "\n",
    "    for title, page_urls, url in results:\n",
    "        print(count, title, url)\n",
    "        count += 1\n",
    "        unseen.update(page_urls - seen)     # get new url to crawl\n",
    "print('Total time: %.1f s' % (time.time()-t1, ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不用异步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start job  1\n",
      "Job  1  takes  1  s\n",
      "Start job  2\n",
      "Job  2  takes  2  s\n",
      "NO async total time :  3.0010645389556885\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def job(t):\n",
    "    print('Start job ', t)\n",
    "    time.sleep(t)               # wait for \"t\" seconds\n",
    "    print('Job ', t, ' takes ', t, ' s')\n",
    "    \n",
    "def main():\n",
    "    [job(t) for t in range(1, 3)]\n",
    "\n",
    "t1 = time.time()\n",
    "main()\n",
    "print(\"NO async total time : \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用异步\n",
    "[参考](https://github.com/MorvanZhou/easy-scraping-tutorial/blob/master/notebook/4-2-asyncio.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def job(t):\n",
    "    print('Start job ', t)\n",
    "    await asyncio.sleep(t)          # wait for \"t\" seconds, it will look for another job while await\n",
    "    print('Job ', t, ' takes ', t, ' s')\n",
    "\n",
    "async def main(loop):\n",
    "    tasks = [loop.create_task(job(t)) for t in range(1, 3)]     # just create, not run job\n",
    "    await asyncio.wait(tasks)                                   # run jobs and wait for all tasks done\n",
    "\n",
    "t1 = time.time()\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main(loop))\n",
    "\n",
    "# loop.close()                          # Ipython notebook gives error if close loop\n",
    "print(\"Async total time : \", time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果：\n",
    "<img src='p10.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium tutorial\n",
    "[https://github.com/MorvanZhou/easy-scraping-tutorial/blob/master/notebook/5-1-selenium.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./img3/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOMO\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# http://selenium-python.readthedocs.io/installation.html 进入这个网址下载驱动\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于下面这些代码是怎么写的，需要用一个火狐浏览器插件，[下载网址](https://addons.mozilla.org/en-US/firefox/addon/katalon-automation-record/)，这个插件需要先人为模拟点击，然后记录，生成python文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"><html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"zh-CN en\"><head>\n",
      "  <meta charset=\"UTF-8\" />\n",
      "  <meta name=\"vie\n"
     ]
    }
   ],
   "source": [
    "# windows用户指导：https://www.cnblogs.com/yqpy/p/8306066.html\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://morvanzhou.github.io/\")\n",
    "driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()\n",
    "driver.find_element_by_link_text(\"About\").click()\n",
    "driver.find_element_by_link_text(u\"赞助\").click()\n",
    "driver.find_element_by_link_text(u\"教程 ▾\").click()\n",
    "driver.find_element_by_link_text(u\"数据处理 ▾\").click()\n",
    "driver.find_element_by_link_text(u\"网页爬虫\").click()\n",
    "\n",
    "html = driver.page_source       # get html\n",
    "driver.get_screenshot_as_file(\"./img/sreenshot1.png\")\n",
    "driver.close()\n",
    "print(html[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果不想让网页真正展示出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"><html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"zh-CN en\"><head>\n",
      "  <meta charset=\"UTF-8\" />\n",
      "  <meta name=\"vie\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")       # define headless\n",
    "\n",
    "# add the option when creating driver\n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)    \n",
    "driver.get(\"https://morvanzhou.github.io/\")\n",
    "driver.find_element_by_xpath(u\"//img[@alt='强化学习 (Reinforcement Learning)']\").click()\n",
    "driver.find_element_by_link_text(\"About\").click()\n",
    "driver.find_element_by_link_text(u\"赞助\").click()\n",
    "driver.find_element_by_link_text(u\"教程 ▾\").click()\n",
    "driver.find_element_by_link_text(u\"数据处理 ▾\").click()\n",
    "driver.find_element_by_link_text(u\"网页爬虫\").click()\n",
    "\n",
    "html = driver.page_source           # get html\n",
    "driver.get_screenshot_as_file(\"./img/sreenshot2.png\") # 确认真正到了这个地址\n",
    "driver.close()\n",
    "print(html[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy框架\n",
    "\n",
    "[scrapy.org](https://scrapy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class MofanSpider(scrapy.Spider):\n",
    "    name = \"mofan\"\n",
    "    start_urls = [\n",
    "        'https://morvanzhou.github.io/',\n",
    "    ]\n",
    "    # unseen = set()\n",
    "    # seen = set()      # we don't need these two as scrapy will deal with them automatically\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield {     # return some results\n",
    "            'title': response.css('h1::text').extract_first(default='Missing').strip().replace('\"', \"\"),\n",
    "            'url': response.url,\n",
    "        }\n",
    "\n",
    "        urls = response.css('a::attr(href)').re(r'^/.+?/$')     # find all sub urls\n",
    "        for url in urls:\n",
    "            yield response.follow(url, callback=self.parse)     # it will filter duplication automatically\n",
    "\n",
    "# 在terminal中输入 scrapy runspider 5-2-scrapy.py -o res.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
